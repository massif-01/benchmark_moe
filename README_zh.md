# benchmark_moe

[ğŸ‡ºğŸ‡¸ English](README.md) | [ğŸ‡¨ğŸ‡³ ä¸­æ–‡](README_zh.md)

åŸºäº vLLM çš„ MoE (Mixture of Experts) æ¨¡å‹å†…æ ¸æ€§èƒ½ä¼˜åŒ–å·¥å…·

[![License: Apache-2.0](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](LICENSE)
[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![vLLM](https://img.shields.io/badge/vLLM-0.10.0+-green.svg)](https://github.com/vllm-project/vllm)

ä¸€ä¸ªä¸“é—¨ç”¨äºä¼˜åŒ– vLLM æ¡†æ¶ä¸­ MoE æ¨¡å‹æ¨ç†æ€§èƒ½çš„å·¥å…·é›†ï¼Œé€šè¿‡è‡ªåŠ¨åŒ–è°ƒä¼˜ Triton å†…æ ¸å‚æ•°ï¼Œä¸ºä¸åŒçš„æ¨¡å‹æ¶æ„å’Œç¡¬ä»¶é…ç½®æ‰¾åˆ°æœ€ä¼˜çš„æ‰§è¡Œé…ç½®ã€‚

## ğŸ¯ ä¸»è¦åŠŸèƒ½

- **ğŸ”§ è‡ªåŠ¨åŒ–å†…æ ¸è°ƒä¼˜**: ä½¿ç”¨ Ray åˆ†å¸ƒå¼æ¡†æ¶è‡ªåŠ¨æœç´¢æœ€ä¼˜çš„ Triton å†…æ ¸é…ç½®
- **ğŸ“Š å¤šæ¨¡å‹æ”¯æŒ**: æ”¯æŒ Mixtralã€DeepSeekã€Qwenã€Jamba ç­‰ä¸»æµ MoE æ¨¡å‹
- **âš¡ æ€§èƒ½ä¼˜åŒ–**: é’ˆå¯¹ä¸åŒæ‰¹æ¬¡å¤§å°å’Œç¡¬ä»¶é…ç½®è¿›è¡Œä¸“é—¨ä¼˜åŒ–
- **ğŸ› ï¸ æ•…éšœè¯Šæ–­**: æä¾›å®Œå–„çš„ç¯å¢ƒæ£€æŸ¥å’Œé—®é¢˜æ’æŸ¥å·¥å…·
- **ğŸ“ˆ ç»“æœåˆ†æ**: ç”Ÿæˆè¯¦ç»†çš„æ€§èƒ½æŠ¥å‘Šå’Œé…ç½®æ¨è

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å‰ç½®è¦æ±‚

- **ç¡¬ä»¶**: NVIDIA GPU (æ¨è A100/H100)
- **è½¯ä»¶**: Ubuntu 18.04+, Python 3.11+, CUDA 11.8+
- **ä¾èµ–**: vLLM 0.10.0+, PyTorch 2.0+, Ray

### å®‰è£…æ­¥éª¤

1. **å…‹éš†é¡¹ç›®**
   ```bash
   git clone https://github.com/your_username/benchmark_moe.git
   cd benchmark_moe
   ```

2. **ç¯å¢ƒæ£€æŸ¥**
   ```bash
   bash scripts/server_check.sh
   ```

3. **è¿è¡Œå•ä¸ªæ¨¡å‹è°ƒä¼˜**
   ```bash
   # åŸºæœ¬è°ƒä¼˜ - Qwen3 æ¨¡å‹
   python benchmark_moe.py \
     --model /path/to/your/qwen3-model \
     --tp-size 1 \
     --dtype auto \
     --batch-size 1 2 4 8 16 32 64 128 \
     --tune \
     --save-dir ./optimized_configs \
     --trust-remote-code
   ```

4. **æŸ¥çœ‹ç»“æœ**
   ```bash
   ls ./optimized_configs/
   # è¾“å‡º: E64N9472_tp1_fp16.json (ç¤ºä¾‹é…ç½®æ–‡ä»¶)
   ```

## ğŸ“‹ è¯¦ç»†ä½¿ç”¨æŒ‡å—

### ç¯å¢ƒå‡†å¤‡

#### æ£€æŸ¥ç³»ç»Ÿç¯å¢ƒ
```bash
# è¿è¡Œç¯å¢ƒæ£€æŸ¥è„šæœ¬
bash scripts/server_check.sh

# æ£€æŸ¥ GPU çŠ¶æ€
nvidia-smi

# æ£€æŸ¥ Python ä¾èµ–
python -c "import vllm, ray, torch, triton; print('ç¯å¢ƒæ£€æŸ¥é€šè¿‡')"
```

#### å¤„ç†å¸¸è§ç¯å¢ƒé—®é¢˜

**é—®é¢˜ 1: Triton ç¼“å­˜æŸå**
```bash
# æ¸…ç† Triton ç¼“å­˜ï¼ˆå¦‚æœé‡åˆ° JSONDecodeErrorï¼‰
rm -rf ~/.triton/cache/*

# æˆ–è®¾ç½®æ–°çš„ç¼“å­˜ç›®å½•
export TRITON_CACHE_DIR=/tmp/triton_cache_$(date +%s)
mkdir -p $TRITON_CACHE_DIR
```

**é—®é¢˜ 2: libstdc++ ç‰ˆæœ¬é—®é¢˜**
```bash
# æ›´æ–° conda ç¯å¢ƒä¸­çš„ libstdc++
conda install -c conda-forge libstdcxx-ng

# æˆ–ä½¿ç”¨ç³»ç»Ÿåº“
export LD_LIBRARY_PATH=/usr/lib/x86_64-linux-gnu:$LD_LIBRARY_PATH
```

**é—®é¢˜ 3: Ray è­¦å‘Š**
```bash
# æ¶ˆé™¤ Ray ç›¸å…³è­¦å‘Š
export RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0
export RAY_DISABLE_IMPORT_WARNING=1
```

### è°ƒä¼˜å‚æ•°è¯´æ˜

#### åŸºæœ¬å‚æ•°
- `--model`: æ¨¡å‹è·¯å¾„æˆ– HuggingFace æ¨¡å‹å
- `--tp-size`: å¼ é‡å¹¶è¡Œåº¦ï¼ˆæ ¹æ® GPU æ•°é‡è®¾ç½®ï¼‰
- `--dtype`: æ•°æ®ç±»å‹ (`auto`, `fp8_w8a8`, `int8_w8a16`)
- `--batch-size`: è¦æµ‹è¯•çš„æ‰¹æ¬¡å¤§å°åˆ—è¡¨
- `--tune`: å¯ç”¨è°ƒä¼˜æ¨¡å¼
- `--save-dir`: é…ç½®æ–‡ä»¶ä¿å­˜ç›®å½•

#### é«˜çº§å‚æ•°
- `--use-deep-gemm`: å¯ç”¨ DeepGEMM ä¼˜åŒ–
- `--enable-expert-parallel`: å¯ç”¨ä¸“å®¶å¹¶è¡Œï¼ˆé€‚ç”¨äºå¤§å‹æ¨¡å‹ï¼‰
- `--seed`: éšæœºç§å­ï¼ˆä¿è¯ç»“æœå¯é‡ç°ï¼‰

### æ”¯æŒçš„æ¨¡å‹ç±»å‹

| æ¨¡å‹ç³»åˆ— | ä¸“å®¶æ•° | Top-K | æ¨èæ˜¾å­˜ | ç¤ºä¾‹å‘½ä»¤ |
|---------|--------|-------|----------|----------|
| **Qwen3-30B-A3B** | 64 | 4 | 64GB+ | `--model path/to/qwen3 --tp-size 1` |
| **Mixtral-8x7B** | 8 | 2 | 45GB+ | `--model mistralai/Mixtral-8x7B-Instruct-v0.1 --tp-size 2` |
| **DeepSeek-V2** | 160 | 6 | 80GB+ | `--model deepseek-ai/DeepSeek-V2-Chat --tp-size 4` |
| **DeepSeek-V3** | 256 | 8 | 120GB+ | `--model deepseek-ai/DeepSeek-V3-Base --tp-size 8` |

### æ‰¹é‡è°ƒä¼˜è„šæœ¬

#### ä½¿ç”¨é…ç½®ç®¡ç†å·¥å…·
```bash
# åˆ—å‡ºæ”¯æŒçš„æ¨¡å‹
python tools/config_manager.py list

# ä¸ºç‰¹å®šæ¨¡å‹è°ƒä¼˜
python tools/config_manager.py tune qwen3_30b

# æŸ¥çœ‹é…ç½®æ¨è
python tools/config_manager.py recommend qwen3_30b
```

#### å®‰å…¨çš„æ‰¹é‡è°ƒä¼˜
```bash
# ä½¿ç”¨å®‰å…¨è„šæœ¬é€ä¸ªæµ‹è¯•æ‰¹æ¬¡å¤§å°
bash scripts/run_benchmark_safe.sh
```

## ğŸ“Š ç»“æœè§£è¯»

### é…ç½®æ–‡ä»¶æ ¼å¼
```json
{
  "triton_version": "2.1.0",
  "1": {                    // batch_size=1 çš„æœ€ä¼˜é…ç½®
    "BLOCK_SIZE_M": 16,
    "BLOCK_SIZE_N": 64, 
    "BLOCK_SIZE_K": 128,
    "GROUP_SIZE_M": 1,
    "num_warps": 4,
    "num_stages": 3
  },
  "64": {                   // batch_size=64 çš„æœ€ä¼˜é…ç½®
    "BLOCK_SIZE_M": 128,
    "BLOCK_SIZE_N": 128,
    "BLOCK_SIZE_K": 256,
    "GROUP_SIZE_M": 32,
    "num_warps": 8,
    "num_stages": 4
  }
}
```

### æ€§èƒ½åˆ†æ
```bash
# è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•ï¼ˆä¸åŠ  --tuneï¼‰
python benchmark_moe.py \
  --model your_model \
  --tp-size 1 \
  --batch-size 1 2 4 8 16 32 64 128

# è¾“å‡ºç¤ºä¾‹:
# Batch size: 1, Kernel time: 45.23 us
# Batch size: 64, Kernel time: 892.15 us
```

## ğŸ› ï¸ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜åŠè§£å†³æ–¹æ¡ˆ

#### 1. å†…å­˜ä¸è¶³é”™è¯¯
```bash
# ç—‡çŠ¶: CUDA out of memory
# è§£å†³æ–¹æ¡ˆ:
# - å‡å°‘æ‰¹æ¬¡å¤§å°
--batch-size 1 2 4 8 16 32

# - ä½¿ç”¨é‡åŒ–
--dtype fp8_w8a8

# - å¢åŠ å¼ é‡å¹¶è¡Œåº¦ï¼ˆå¦‚æœæœ‰å¤šGPUï¼‰
--tp-size 2
```

#### 2. Triton ç¼–è¯‘é”™è¯¯
```bash
# ç—‡çŠ¶: JSONDecodeError, OutOfResources
# è§£å†³æ–¹æ¡ˆ:
rm -rf ~/.triton/cache/*
export TRITON_CACHE_DIR=/tmp/triton_cache_new
```

#### 3. æ¨¡å‹åŠ è½½å¤±è´¥
```bash
# ç—‡çŠ¶: Model not found, Permission denied
# è§£å†³æ–¹æ¡ˆ:
# - æ£€æŸ¥æ¨¡å‹è·¯å¾„
ls /path/to/your/model

# - æ·»åŠ è®¿é—®æƒé™
--trust-remote-code

# - é¢„ä¸‹è½½æ¨¡å‹
huggingface-cli download model_name --local-dir ./models/
```

#### 4. Ray åˆå§‹åŒ–é—®é¢˜
```bash
# ç—‡çŠ¶: Ray æ— æ³•å¯åŠ¨
# è§£å†³æ–¹æ¡ˆ:
export RAY_DISABLE_IMPORT_WARNING=1
ray stop  # åœæ­¢ç°æœ‰å®ä¾‹
ray start --head  # é‡æ–°å¯åŠ¨
```

### æ€§èƒ½è°ƒä¼˜å»ºè®®

#### é’ˆå¯¹ä¸åŒä½¿ç”¨åœºæ™¯

**ä½å»¶è¿Ÿåœºæ™¯ï¼ˆåœ¨çº¿æ¨ç†ï¼‰**
```bash
# ä¼˜åŒ–å°æ‰¹æ¬¡æ€§èƒ½
--batch-size 1 2 4 8 16
--dtype fp8_w8a8  # å‡å°‘å†…å­˜è®¿é—®å»¶è¿Ÿ
```

**é«˜åååœºæ™¯ï¼ˆæ‰¹é‡å¤„ç†ï¼‰**
```bash
# ä¼˜åŒ–å¤§æ‰¹æ¬¡æ€§èƒ½  
--batch-size 64 128 256 512 1024
--dtype auto  # å¹³è¡¡ç²¾åº¦å’Œæ€§èƒ½
```

**å†…å­˜å—é™åœºæ™¯**
```bash
# æœ€å¤§åŒ–å†…å­˜åˆ©ç”¨ç‡
--dtype fp8_w8a8
--use-deep-gemm
--enable-expert-parallel
```

```
benchmark_moe/
â”œâ”€â”€ README.md                   # é¡¹ç›®è¯´æ˜æ–‡æ¡£
â”œâ”€â”€ benchmark_moe.py           # vLLM MoE åŸºå‡†æµ‹è¯•æ ¸å¿ƒè„šæœ¬
â”œâ”€â”€ scripts/                   # è¿è¡Œè„šæœ¬ç›®å½•
â”‚   â”œâ”€â”€ server_check.sh        # æœåŠ¡å™¨ç¯å¢ƒæ£€æŸ¥è„šæœ¬
â”‚   â”œâ”€â”€ tune_mixtral.sh        # Mixtral æ¨¡å‹è°ƒä¼˜è„šæœ¬
â”‚   â”œâ”€â”€ tune_deepseek.sh       # DeepSeek æ¨¡å‹è°ƒä¼˜è„šæœ¬
â”‚   â””â”€â”€ run_benchmark_safe.sh  # å®‰å…¨çš„æ‰¹é‡è°ƒä¼˜è„šæœ¬
â”œâ”€â”€ configs/                   # é…ç½®æ–‡ä»¶ç›®å½•
â”‚   â”œâ”€â”€ models.json            # æ”¯æŒçš„æ¨¡å‹é…ç½®
â”‚   â””â”€â”€ tuning_params.json     # è°ƒä¼˜å‚æ•°é…ç½®
â”œâ”€â”€ tools/                     # åˆ†æå·¥å…·ç›®å½•
â”‚   â””â”€â”€ config_manager.py      # é…ç½®ç®¡ç†å·¥å…·
â”œâ”€â”€ results/                   # ç»“æœè¾“å‡ºç›®å½•
â”‚   â”œâ”€â”€ tuned_configs/         # è°ƒä¼˜åçš„é…ç½®æ–‡ä»¶
â”‚   â””â”€â”€ performance_reports/   # æ€§èƒ½æµ‹è¯•æŠ¥å‘Š
â””â”€â”€ deployment/                # éƒ¨ç½²ç›¸å…³æ–‡ä»¶
    â”œâ”€â”€ requirements.txt       # Pythonä¾èµ–åˆ—è¡¨
    â””â”€â”€ DEPLOYMENT_GUIDE.md    # è¯¦ç»†éƒ¨ç½²æŒ‡å—
```

## ğŸ¤ è´¡çŒ®æŒ‡å—

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼

### å¼€å‘ç¯å¢ƒè®¾ç½®
```bash
git clone https://github.com/your_username/benchmark_moe.git
cd benchmark_moe
pip install -r deployment/requirements.txt
```

### æäº¤ä»£ç 
1. Fork è¿™ä¸ªé¡¹ç›®
2. åˆ›å»ºæ‚¨çš„ç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤æ‚¨çš„æ”¹åŠ¨ (`git commit -m 'Add some AmazingFeature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. å¼€å¯ä¸€ä¸ª Pull Request

## ğŸ“„ è®¸å¯è¯

æ­¤é¡¹ç›®é‡‡ç”¨ Apache-2.0 è®¸å¯è¯ã€‚è¯¦æƒ…è¯·è§ [LICENSE](LICENSE) æ–‡ä»¶ã€‚

## ï¿½ è‡´è°¢

- [vLLM](https://github.com/vllm-project/vllm) - é«˜æ€§èƒ½ LLM æ¨ç†å¼•æ“
- [Ray](https://github.com/ray-project/ray) - åˆ†å¸ƒå¼è®¡ç®—æ¡†æ¶
- [Triton](https://github.com/openai/triton) - GPU ç¼–ç¨‹è¯­è¨€

## ğŸ“® è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼è”ç³»ï¼š

- æäº¤ [GitHub Issue](https://github.com/your_username/benchmark_moe/issues)
- å‘èµ· [Discussion](https://github.com/your_username/benchmark_moe/discussions)

---

**â­ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œè¯·ç»™æˆ‘ä»¬ä¸€ä¸ª Starï¼**
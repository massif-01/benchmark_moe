# vLLM MoE Benchmark éƒ¨ç½²æŒ‡å—

[ğŸ‡ºğŸ‡¸ English](DEPLOYMENT_GUIDE.md) | [ğŸ‡¨ğŸ‡³ ä¸­æ–‡](DEPLOYMENT_GUIDE_zh.md)

æœ¬æ–‡æ¡£æä¾›äº†è¯¦ç»†çš„éƒ¨ç½²å’Œæ•…éšœæ’é™¤æŒ‡å—ã€‚

## ç³»ç»Ÿè¦æ±‚

### ç¡¬ä»¶è¦æ±‚
- **GPU**: NVIDIA GPU with CUDA Compute Capability 7.0+ (æ¨è A100/H100)
- **æ˜¾å­˜**: è‡³å°‘ 40GB (æ ¹æ®æ¨¡å‹å¤§å°è°ƒæ•´)
- **å†…å­˜**: è‡³å°‘ 32GB RAM
- **å­˜å‚¨**: è‡³å°‘ 100GB å¯ç”¨ç©ºé—´

### è½¯ä»¶è¦æ±‚
- **æ“ä½œç³»ç»Ÿ**: Ubuntu 18.04+ / CentOS 7+ / RHEL 7+
- **Python**: 3.11+
- **CUDA**: 11.8+ (æ¨è 12.1+)
- **Docker**: å¯é€‰ï¼Œç”¨äºå®¹å™¨åŒ–éƒ¨ç½²

## å®‰è£…æ­¥éª¤

### 1. ç¯å¢ƒå‡†å¤‡

#### æ£€æŸ¥ CUDA ç¯å¢ƒ
```bash
nvidia-smi
nvcc --version
```

#### åˆ›å»º Python ç¯å¢ƒ
```bash
# ä½¿ç”¨ conda (æ¨è)
conda create -n benchmark_moe python=3.11
conda activate benchmark_moe

# æˆ–ä½¿ç”¨ venv
python3.11 -m venv benchmark_moe_env
source benchmark_moe_env/bin/activate
```

### 2. å®‰è£…ä¾èµ–

#### åŸºç¡€ä¾èµ–
```bash
# å®‰è£… PyTorch (CUDA ç‰ˆæœ¬)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# å®‰è£… vLLM
pip install vllm

# å®‰è£…å…¶ä»–ä¾èµ–
pip install -r deployment/requirements.txt
```

#### éªŒè¯å®‰è£…
```bash
python -c "
import torch
import vllm
import ray
import triton
print('âœ… æ‰€æœ‰ä¾èµ–å®‰è£…æˆåŠŸ')
print(f'PyTorch: {torch.__version__}')
print(f'CUDA å¯ç”¨: {torch.cuda.is_available()}')
print(f'vLLM: {vllm.__version__}')
"
```

### 3. æ¨¡å‹å‡†å¤‡

#### ä¸‹è½½æ¨¡å‹
```bash
# ä½¿ç”¨ huggingface-cli ä¸‹è½½
huggingface-cli download mistralai/Mixtral-8x7B-Instruct-v0.1 --local-dir ./models/mixtral-8x7b

# æˆ–è€…ä½¿ç”¨ Python è„šæœ¬
python -c "
from transformers import AutoTokenizer, AutoConfig
model_name = 'mistralai/Mixtral-8x7B-Instruct-v0.1'
tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir='./models')
config = AutoConfig.from_pretrained(model_name, cache_dir='./models')
print('æ¨¡å‹é…ç½®ä¸‹è½½å®Œæˆ')
"
```

## é…ç½®ä¼˜åŒ–

### ç¯å¢ƒå˜é‡è®¾ç½®
```bash
# æ·»åŠ åˆ° ~/.bashrc æˆ– ~/.zshrc
export CUDA_VISIBLE_DEVICES=0  # æ ¹æ®å¯ç”¨ GPU è°ƒæ•´
export RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0
export RAY_DISABLE_IMPORT_WARNING=1
export TRITON_CACHE_DIR=/tmp/triton_cache
```

### å†…å­˜ä¼˜åŒ–
```bash
# è®¾ç½® GPU å†…å­˜åˆ†é…ç­–ç•¥
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128

# å¯ç”¨å†…å­˜æ˜ å°„
export VLLM_USE_MODELSCOPE=0
export VLLM_ALLOW_RUNTIME_LORA_UPDATING=1
```

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### 1. CUDA ç›¸å…³é”™è¯¯

**é”™è¯¯**: `CUDA out of memory`
```bash
# è§£å†³æ–¹æ¡ˆ:
# - å‡å°‘æ‰¹æ¬¡å¤§å°
python benchmark_moe.py --batch-size 1 2 4 8

# - ä½¿ç”¨é‡åŒ–
python benchmark_moe.py --dtype fp8_w8a8

# - æ¸…ç† GPU å†…å­˜
nvidia-smi --gpu-reset
```

**é”™è¯¯**: `CUDA driver version is insufficient`
```bash
# æ£€æŸ¥é©±åŠ¨ç‰ˆæœ¬
nvidia-smi

# æ›´æ–° NVIDIA é©±åŠ¨
sudo ubuntu-drivers autoinstall
sudo reboot
```

#### 2. Triton ç¼–è¯‘é”™è¯¯

**é”™è¯¯**: `JSONDecodeError: Expecting value`
```bash
# æ¸…ç† Triton ç¼“å­˜
rm -rf ~/.triton/cache/*
rm -rf /tmp/triton_cache*

# è®¾ç½®æ–°çš„ç¼“å­˜ç›®å½•
export TRITON_CACHE_DIR=/tmp/triton_cache_$(date +%s)
mkdir -p $TRITON_CACHE_DIR
```

**é”™è¯¯**: `OutOfResources`
```bash
# å‡å°‘æœç´¢ç©ºé—´
python benchmark_moe.py --batch-size 1 2 4  # è¾ƒå°çš„æ‰¹æ¬¡èŒƒå›´

# æˆ–ä½¿ç”¨å¿«é€Ÿæ¨¡å¼
bash scripts/run_benchmark_safe.sh --batch-sizes 1,2,4,8
```

#### 3. Ray ç›¸å…³é—®é¢˜

**é”™è¯¯**: `Ray cluster initialization failed`
```bash
# åœæ­¢ç°æœ‰ Ray è¿›ç¨‹
ray stop

# é‡æ–°å¯åŠ¨
ray start --head --port=6379

# æˆ–è€…æ¸…ç† Ray ä¸´æ—¶æ–‡ä»¶
rm -rf /tmp/ray*
```

#### 4. æ¨¡å‹åŠ è½½é”™è¯¯

**é”™è¯¯**: `Model not found` æˆ– `Permission denied`
```bash
# æ£€æŸ¥æ¨¡å‹è·¯å¾„
ls -la /path/to/your/model

# è®¾ç½®æ­£ç¡®æƒé™
chmod -R 755 /path/to/your/model

# ä½¿ç”¨ä¿¡ä»»è¿œç¨‹ä»£ç 
python benchmark_moe.py --trust-remote-code
```

**é”™è¯¯**: `Tokenizer not found`
```bash
# é¢„ä¸‹è½½ tokenizer
python -c "
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained('your_model_name')
"
```

#### 5. ç½‘ç»œç›¸å…³é—®é¢˜

**é”™è¯¯**: `Connection timeout` (è®¿é—® HuggingFace)
```bash
# è®¾ç½®ä»£ç† (å¦‚éœ€è¦)
export HTTP_PROXY=http://proxy.example.com:8080
export HTTPS_PROXY=http://proxy.example.com:8080

# æˆ–ä½¿ç”¨é•œåƒ
export HF_ENDPOINT=https://hf-mirror.com
```

### æ€§èƒ½è°ƒä¼˜å»ºè®®

#### é’ˆå¯¹ä¸åŒç¡¬ä»¶çš„ä¼˜åŒ–

**å• GPU (A100 80GB)**
```bash
python benchmark_moe.py \
  --tp-size 1 \
  --dtype fp8_w8a8 \
  --batch-size 1 2 4 8 16 32 64 128
```

**å¤š GPU (2x A100)**
```bash
python benchmark_moe.py \
  --tp-size 2 \
  --dtype auto \
  --batch-size 16 32 64 128 256 512
```

**å†…å­˜å—é™ç¯å¢ƒ**
```bash
python benchmark_moe.py \
  --tp-size 1 \
  --dtype fp8_w8a8 \
  --batch-size 1 2 4 8 \
  --use-deep-gemm
```

## ç›‘æ§å’Œè°ƒè¯•

### æ€§èƒ½ç›‘æ§
```bash
# GPU ä½¿ç”¨ç›‘æ§
watch -n 1 nvidia-smi

# å†…å­˜ä½¿ç”¨ç›‘æ§
htop

# ç½‘ç»œ I/O ç›‘æ§
iotop
```

### è°ƒè¯•æ—¥å¿—
```bash
# å¯ç”¨è¯¦ç»†æ—¥å¿—
export VLLM_LOGGING_LEVEL=DEBUG
export RAY_VERBOSE=1

# è¿è¡Œè°ƒè¯•æ¨¡å¼
python benchmark_moe.py --batch-size 1 --tune 2>&1 | tee debug.log
```

## å®¹å™¨åŒ–éƒ¨ç½²

### Dockerfile ç¤ºä¾‹
```dockerfile
FROM nvidia/cuda:12.1-devel-ubuntu22.04

RUN apt-get update && apt-get install -y \
    python3.11 python3.11-pip python3.11-dev \
    git wget curl

WORKDIR /app
COPY . .

RUN pip install -r deployment/requirements.txt

CMD ["python", "benchmark_moe.py", "--help"]
```

### Docker è¿è¡Œ
```bash
# æ„å»ºé•œåƒ
docker build -t benchmark_moe .

# è¿è¡Œå®¹å™¨
docker run --gpus all -v /path/to/models:/app/models benchmark_moe \
  python benchmark_moe.py --model /app/models/your_model --tune
```

## æœ€ä½³å®è·µ

### 1. èµ„æºç®¡ç†
- ä½¿ç”¨ `nvidia-smi` å®šæœŸæ£€æŸ¥ GPU ä½¿ç”¨æƒ…å†µ
- è®¾ç½®åˆé€‚çš„æ‰¹æ¬¡å¤§å°é¿å…å†…å­˜æº¢å‡º
- å®šæœŸæ¸…ç† Triton ç¼“å­˜é¿å…ç£ç›˜ç©ºé—´ä¸è¶³

### 2. è°ƒä¼˜ç­–ç•¥
- ä»å°æ‰¹æ¬¡å¼€å§‹é€æ­¥å¢åŠ 
- ä¼˜å…ˆæµ‹è¯•å¸¸ç”¨çš„æ‰¹æ¬¡å¤§å°
- ä¿å­˜è°ƒä¼˜ç»“æœä»¥ä¾›åç»­ä½¿ç”¨

### 3. ç”Ÿäº§éƒ¨ç½²
- ä½¿ç”¨ç¨³å®šçš„æ¨¡å‹ç‰ˆæœ¬
- è®¾ç½®åˆé€‚çš„è¶…æ—¶æ—¶é—´
- å»ºç«‹ç›‘æ§å’Œå‘Šè­¦æœºåˆ¶

## æŠ€æœ¯æ”¯æŒ

å¦‚æœé‡åˆ°é—®é¢˜ï¼Œè¯·æŒ‰ä»¥ä¸‹é¡ºåºæ’æŸ¥ï¼š

1. **æ£€æŸ¥ç¯å¢ƒ**: è¿è¡Œ `bash scripts/server_check.sh`
2. **æŸ¥çœ‹æ—¥å¿—**: æ£€æŸ¥é”™è¯¯ä¿¡æ¯å’Œå †æ ˆè·Ÿè¸ª
3. **æœç´¢å·²çŸ¥é—®é¢˜**: æŸ¥çœ‹é¡¹ç›® Issues é¡µé¢
4. **åˆ›å»ºæ–° Issue**: æä¾›è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯å’Œç¯å¢ƒé…ç½®

### æŠ¥å‘Šé—®é¢˜æ—¶è¯·åŒ…å«ï¼š
- æ“ä½œç³»ç»Ÿå’Œç‰ˆæœ¬
- GPU å‹å·å’Œé©±åŠ¨ç‰ˆæœ¬
- Python å’Œä¾èµ–åŒ…ç‰ˆæœ¬
- å®Œæ•´çš„é”™è¯¯æ—¥å¿—
- é‡ç°æ­¥éª¤